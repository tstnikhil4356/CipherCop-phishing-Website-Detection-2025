{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f268a1a-62c6-45a8-92e8-4fc52e83b6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:24:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4279, number of negative: 4293\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 916\n",
      "[LightGBM] [Info] Number of data points in the train set: 8572, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499183 -> initscore=-0.003266\n",
      "[LightGBM] [Info] Start training from score -0.003266\n",
      "‚úÖ Model, scaler, and features saved to phishing_lgbm.pkl\n",
      "URL: https://jodp.asia/dakao/\n",
      "Prediction: üö® Phishing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Step 3: Load Dataset\n",
    "df = pd.read_csv('dataset_phishing.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 4: Feature Selection\n",
    "features = [\n",
    "    'length_url', 'length_hostname', 'ip', 'nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq',\n",
    "    'nb_underscore', 'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star', 'nb_colon', 'nb_comma', 'nb_semicolumn',\n",
    "    'nb_dollar', 'nb_space', 'nb_www', 'nb_com', 'nb_dslash', 'http_in_path', 'https_token', 'ratio_digits_url',\n",
    "    'ratio_digits_host', 'punycode', 'shortening_service', 'path_extension', 'phish_hints', 'domain_in_brand',\n",
    "    'brand_in_subdomain', 'brand_in_path', 'suspecious_tld'\n",
    "]\n",
    "\n",
    "df['status'] = df['status'].map({'phishing': 1, 'legitimate': 0})\n",
    "\n",
    "X = df[features]\n",
    "y = df['status']\n",
    "\n",
    "# Step 5: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Step 6: Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 7: Train Best Model (Voting Classifier)\n",
    "best_model = VotingClassifier(estimators=[\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "    ('catboost', CatBoostClassifier(verbose=0)),\n",
    "    ('lgbm', LGBMClassifier()),\n",
    "    ('gb', GradientBoostingClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('svm', SVC(probability=True))\n",
    "], voting='soft')\n",
    "\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ---------- SAVE MODEL PACKAGE ----------\n",
    "model_package = {\n",
    "    \"model\": best_model,\n",
    "    \"scaler\": scaler,\n",
    "    \"features\": features\n",
    "}\n",
    "\n",
    "with open(\"phishing_lgbm.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "print(\"‚úÖ Model, scaler, and features saved to phishing_lgbm.pkl\")\n",
    "\n",
    "# ---------- URL Feature Extraction ----------\n",
    "def extract_features(url):\n",
    "    \"\"\"Extract minimal features from a URL (must match training features).\"\"\"\n",
    "    features = {}\n",
    "    features['length_url'] = len(url)\n",
    "    features['length_hostname'] = len(re.findall(r'://([^/]+)/?', url)[0]) if \"://\" in url else len(url)\n",
    "    features['ip'] = 1 if re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', url) else 0\n",
    "    features['nb_dots'] = url.count('.')\n",
    "    features['nb_hyphens'] = url.count('-')\n",
    "    features['nb_at'] = url.count('@')\n",
    "    features['nb_qm'] = url.count('?')\n",
    "    features['nb_and'] = url.count('&')\n",
    "    features['nb_or'] = url.count('|')\n",
    "    features['nb_eq'] = url.count('=')\n",
    "    features['nb_underscore'] = url.count('_')\n",
    "    features['nb_tilde'] = url.count('~')\n",
    "    features['nb_percent'] = url.count('%')\n",
    "    features['nb_slash'] = url.count('/')\n",
    "    features['nb_star'] = url.count('*')\n",
    "    features['nb_colon'] = url.count(':')\n",
    "    features['nb_comma'] = url.count(',')\n",
    "    features['nb_semicolumn'] = url.count(';')\n",
    "    features['nb_dollar'] = url.count('$')\n",
    "    features['nb_space'] = url.count(' ')\n",
    "    features['nb_www'] = url.count('www')\n",
    "    features['nb_com'] = url.count('.com')\n",
    "    features['nb_dslash'] = url.count('//')\n",
    "    features['http_in_path'] = 1 if \"http\" in url[url.find(\"://\")+3:] else 0\n",
    "    features['https_token'] = 1 if \"https\" in url else 0\n",
    "    features['ratio_digits_url'] = sum(c.isdigit() for c in url) / len(url)\n",
    "    features['ratio_digits_host'] = 0.0  # simplified placeholder\n",
    "    features['punycode'] = 1 if \"xn--\" in url else 0\n",
    "    features['shortening_service'] = 1 if re.search(r'bit\\.ly|goo\\.gl|tinyurl|ow\\.ly', url) else 0\n",
    "    features['path_extension'] = 1 if re.search(r'\\.[a-zA-Z0-9]{2,4}(/|$)', url) else 0\n",
    "    features['phish_hints'] = 1 if re.search(r'login|verify|bank|account|update|secure', url.lower()) else 0\n",
    "    features['domain_in_brand'] = 0  # need brand dictionary\n",
    "    features['brand_in_subdomain'] = 0\n",
    "    features['brand_in_path'] = 0\n",
    "    features['suspecious_tld'] = 1 if re.search(r'\\.(zip|review|country|kim|cricket|science|work|party)$', url) else 0\n",
    "    return features\n",
    "\n",
    "# ---------- LOAD AND PREDICT ----------\n",
    "def predict_url(url):\n",
    "    # Load model package\n",
    "    with open(\"phishing_lgbm.pkl\", \"rb\") as f:\n",
    "        package = pickle.load(f)\n",
    "\n",
    "    model = package[\"model\"]\n",
    "    scaler = package[\"scaler\"]\n",
    "    features_list = package[\"features\"]\n",
    "\n",
    "    # Extract features\n",
    "    feats = extract_features(url)\n",
    "    X_input = pd.DataFrame([feats])\n",
    "    X_input = X_input.reindex(columns=features_list, fill_value=0)\n",
    "    X_input_scaled = scaler.transform(X_input)\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(X_input_scaled)[0]\n",
    "    return \"üö® Phishing\" if prediction == 1 else \"‚úÖ Safe\"\n",
    "\n",
    "# ---------- Example ----------\n",
    "test_url = \"https://jodp.asia/dakao/\"\n",
    "print(\"URL:\", test_url)\n",
    "print(\"Prediction:\", predict_url(test_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c8c0406-89ae-43a0-a213-e1c83d4d2c23",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m scaler \u001b[38;5;241m=\u001b[39m package[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     16\u001b[0m features_list \u001b[38;5;241m=\u001b[39m package[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 17\u001b[0m lgbm_model \u001b[38;5;241m=\u001b[39m package[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlgbm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# ---------- Feature Extractor ----------\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_features\u001b[39m(url):\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lgbm'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "# ---------- Load LGBM Model Package ----------\n",
    "with open(\"phishing_lgbm.pkl\", \"rb\") as f:\n",
    "    package = pickle.load(f)\n",
    "\n",
    "ml_model = package[\"model\"]\n",
    "scaler = package[\"scaler\"]\n",
    "features_list = package[\"features\"]\n",
    "lgbm_model = package[\"lgbm\"]\n",
    "\n",
    "# ---------- Feature Extractor ----------\n",
    "def extract_features(url):\n",
    "    feats = {}\n",
    "    feats['length_url'] = len(url)\n",
    "    feats['length_hostname'] = len(re.findall(r'://([^/]+)/?', url)[0]) if \"://\" in url else len(url)\n",
    "    feats['ip'] = 1 if re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', url) else 0\n",
    "    feats['nb_dots'] = url.count('.')\n",
    "    feats['nb_hyphens'] = url.count('-')\n",
    "    feats['nb_at'] = url.count('@')\n",
    "    feats['nb_qm'] = url.count('?')\n",
    "    feats['nb_and'] = url.count('&')\n",
    "    feats['nb_or'] = url.count('|')\n",
    "    feats['nb_eq'] = url.count('=')\n",
    "    feats['nb_underscore'] = url.count('_')\n",
    "    feats['nb_tilde'] = url.count('~')\n",
    "    feats['nb_percent'] = url.count('%')\n",
    "    feats['nb_slash'] = url.count('/')\n",
    "    feats['nb_star'] = url.count('*')\n",
    "    feats['nb_colon'] = url.count(':')\n",
    "    feats['nb_comma'] = url.count(',')\n",
    "    feats['nb_semicolumn'] = url.count(';')\n",
    "    feats['nb_dollar'] = url.count('$')\n",
    "    feats['nb_space'] = url.count(' ')\n",
    "    feats['nb_www'] = url.count('www')\n",
    "    feats['nb_com'] = url.count('.com')\n",
    "    feats['nb_dslash'] = url.count('//')\n",
    "    feats['http_in_path'] = 1 if \"http\" in url[url.find(\"://\")+3:] else 0\n",
    "    feats['https_token'] = 1 if \"https\" in url else 0\n",
    "    feats['ratio_digits_url'] = sum(c.isdigit() for c in url) / len(url)\n",
    "    feats['ratio_digits_host'] = 0.0\n",
    "    feats['punycode'] = 1 if \"xn--\" in url else 0\n",
    "    feats['shortening_service'] = 1 if re.search(r'bit\\.ly|goo\\.gl|tinyurl|ow\\.ly', url) else 0\n",
    "    feats['path_extension'] = 1 if re.search(r'\\.[a-zA-Z0-9]{2,4}(/|$)', url) else 0\n",
    "    feats['phish_hints'] = 1 if re.search(r'login|verify|bank|account|update|secure', url.lower()) else 0\n",
    "    feats['domain_in_brand'] = 0\n",
    "    feats['brand_in_subdomain'] = 0\n",
    "    feats['brand_in_path'] = 0\n",
    "    feats['suspecious_tld'] = 1 if re.search(r'\\.(zip|review|country|kim|cricket|science|work|party|info)$', url) else 0\n",
    "    return feats\n",
    "\n",
    "# ---------- SHAP Explainability Formatter ----------\n",
    "def format_shap_explanations(features_list, shap_array):\n",
    "    explanations_pos, explanations_neg = [], []\n",
    "    for feat, val in zip(features_list, shap_array[0]):\n",
    "        if abs(val) < 0.2:  # ignore weak contributions\n",
    "            continue\n",
    "        direction = \"phishing\" if val > 0 else \"legitimate\"\n",
    "        text = f\"{feat.replace('_',' ')} pushes towards {direction}\"\n",
    "        if val > 0:\n",
    "            explanations_pos.append(text)\n",
    "        else:\n",
    "            explanations_neg.append(text)\n",
    "    return explanations_pos, explanations_neg\n",
    "\n",
    "# ---------- Extract Main Content ----------\n",
    "def extract_main_content(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for element in soup([\"script\", \"style\", \"meta\", \"link\", \"header\", \"footer\", \"nav\", \"aside\", \"noscript\"]):\n",
    "        element.decompose()\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    return text\n",
    "\n",
    "# ---------- LLM Analysis (Ollama Mistral) ----------\n",
    "def analyze_with_ollama(content, model_name=\"mistral\"):\n",
    "    system_prompt = \"\"\"You are an expert cybersecurity analyst. \n",
    "Your task is to analyze the content of a website and determine if it is a scam, phishing attempt, or otherwise malicious.\n",
    "Respond STRICTLY in this JSON format:\n",
    "\n",
    "{\n",
    "  \"verdict\": \"phishing\" or \"legitimate\",\n",
    "  \"risk_level\": \"suspicious\" or \"safe\",\n",
    "  \"reasons\": [\"list of brief reasons\"],\n",
    "  \"evidence_snippets\": [\"list of concrete snippets found in the text\"]\n",
    "}\n",
    "\"\"\"\n",
    "    try:\n",
    "        user_message = f\"Analyze this website content:\\n\\n{content}\"\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': user_message}\n",
    "            ],\n",
    "            options={'temperature': 0.1}\n",
    "        )\n",
    "        response_text = response['message']['content'].strip()\n",
    "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            return json.loads(json_match.group())\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with Ollama: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------- Hybrid Classification ----------\n",
    "def classify_content(url):\n",
    "    # Step 1: Fetch page\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Failed to fetch URL: {e}\"}\n",
    "\n",
    "    # Step 2: Extract content\n",
    "    main_text = extract_main_content(html_content)\n",
    "    max_chars = 4000\n",
    "    text_for_llm = (url + \" \" + main_text)[:max_chars]\n",
    "\n",
    "    print(f\"üì° Feeding {len(text_for_llm)} characters to LLM (max={max_chars})\\n\")\n",
    "\n",
    "    # Step 3: ML prediction\n",
    "    feats = extract_features(url)\n",
    "    X_input = pd.DataFrame([feats]).reindex(columns=features_list, fill_value=0)\n",
    "    X_scaled = scaler.transform(X_input)\n",
    "    ml_prob = ml_model.predict_proba(X_scaled)[0][1]\n",
    "    ml_pred = \"phishing\" if ml_prob > 0.5 else \"legitimate\"\n",
    "\n",
    "    # SHAP explanations\n",
    "    explainer = shap.TreeExplainer(lgbm_model)\n",
    "    shap_values = explainer.shap_values(X_scaled)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_array = shap_values[1]\n",
    "    else:\n",
    "        shap_array = shap_values\n",
    "    ml_pos, ml_neg = format_shap_explanations(features_list, shap_array)\n",
    "\n",
    "    # Step 4: LLM prediction\n",
    "    llm_result = analyze_with_ollama(text_for_llm)\n",
    "    if llm_result:\n",
    "        llm_label = llm_result.get(\"verdict\", \"unknown\")\n",
    "        llm_risk = llm_result.get(\"risk_level\", \"suspicious\")\n",
    "        llm_reasons = llm_result.get(\"reasons\", [])\n",
    "        evidence_snippets = llm_result.get(\"evidence_snippets\", [])\n",
    "    else:\n",
    "        llm_label, llm_risk, llm_reasons, evidence_snippets = \"unknown\", \"suspicious\", [], []\n",
    "\n",
    "    # Step 5: Ensemble decision\n",
    "    llm_score = 1.0 if llm_label == \"phishing\" else 0.0 if llm_label == \"legitimate\" else 0.5\n",
    "    if llm_risk == \"suspicious\" and 0.15 < ml_prob < 0.5:\n",
    "        final = \"phishing\"\n",
    "    elif ml_prob >= 0.85:\n",
    "        final = \"phishing\"\n",
    "    elif ml_prob <= 0.15:\n",
    "        final = \"legitimate\"\n",
    "    else:\n",
    "        combined_score = (0.6 * ml_prob) + (0.4 * llm_score)\n",
    "        final = \"phishing\" if combined_score >= 0.5 else \"legitimate\"\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"final_verdict\": final,\n",
    "        \"confidence\": round(float(ml_prob), 3),\n",
    "        \"ml_prediction\": ml_pred,\n",
    "        \"ml_positive\": ml_pos,\n",
    "        \"ml_negative\": ml_neg,\n",
    "        \"llm_prediction\": llm_label,\n",
    "        \"llm_risk_level\": llm_risk,\n",
    "        \"llm_reasons\": llm_reasons,\n",
    "        \"evidence_snippets\": evidence_snippets\n",
    "    }\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main():\n",
    "    print(\"üåê Website Scam & Phishing Analyzer\")\n",
    "    print(\"----------------------------------------\")\n",
    "    website_url = input(\"Please enter the full URL to analyze: \").strip()\n",
    "    result = classify_content(website_url)\n",
    "\n",
    "    if \"error\" in result:\n",
    "        print(f\"‚ùå {result['error']}\")\n",
    "        return\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìã URL: {result['url']}\")\n",
    "    verdict_text = 'üî¥ PHISHING' if result['final_verdict'] == 'phishing' else 'üü¢ LEGITIMATE'\n",
    "    print(f\"‚öñÔ∏è Final Verdict: {verdict_text} (confidence={result['confidence']})\")\n",
    "    print(f\"‚úÖ ML Prediction: {result['ml_prediction']}\")\n",
    "    print(f\"ü§ñ LLM Prediction: {result['llm_prediction']} (risk={result['llm_risk_level']})\")\n",
    "\n",
    "    # Unified Analysis\n",
    "    print(\"\\nü§ñ AI's Contextual Analysis:\")\n",
    "\n",
    "    print(\"\\nPhishing Indicators:\")\n",
    "    for r in result['ml_positive'] + result['llm_reasons']:\n",
    "        print(f\" ‚Ä¢ {r}\")\n",
    "    if not result['ml_positive'] and not result['llm_reasons']:\n",
    "        print(\" ‚Ä¢ None detected.\")\n",
    "\n",
    "    print(\"\\nLegitimate Indicators:\")\n",
    "    for r in result['ml_negative']:\n",
    "        print(f\" ‚Ä¢ {r}\")\n",
    "    if not result['ml_negative']:\n",
    "        print(\" ‚Ä¢ None detected.\")\n",
    "\n",
    "    print(\"\\nüîç Concrete Evidence Found in Text:\")\n",
    "    if result['evidence_snippets']:\n",
    "        for e in result['evidence_snippets']:\n",
    "            print(f\" ‚Ä¢ {e}\")\n",
    "    else:\n",
    "        print(\" ‚Ä¢ None detected.\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d9bd4-f428-4562-ad1d-45a282a848ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1434901,
     "sourceId": 2374680,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (virtual_mouse)",
   "language": "python",
   "name": "virtual_mouse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 555.515949,
   "end_time": "2024-09-29T10:55:16.738064",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-29T10:46:01.222115",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
