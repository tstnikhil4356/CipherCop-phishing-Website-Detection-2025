{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d969d7-0030-4906-bb5f-7f2674f70a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Website Scam & Phishing Analyzer\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the full URL to analyze:  https://www.linkedin.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° Feeding 4000 characters to LLM (max=4000)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh\\anaconda3\\Lib\\site-packages\\shap\\explainers\\_tree.py:583: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìã URL: https://www.linkedin.com\n",
      "‚öñÔ∏è Final Verdict: üü¢ LEGITIMATE (confidence=0.224)\n",
      "‚úÖ ML Prediction: legitimate\n",
      "ü§ñ LLM Prediction: legitimate (risk=safe)\n",
      "\n",
      "ü§ñ AI's Contextual Analysis:\n",
      "\n",
      "Phishing Indicators:\n",
      " ‚Ä¢ The website is well-known and reputable (LinkedIn).\n",
      " ‚Ä¢ The content contains clear links to LinkedIn's User Agreement, Privacy Policy, and Cookie Policy.\n",
      " ‚Ä¢ The website provides a legitimate service for professional networking and job seeking.\n",
      "\n",
      "Legitimate Indicators:\n",
      " ‚Ä¢ length url pushes towards legitimate\n",
      " ‚Ä¢ length hostname pushes towards legitimate\n",
      " ‚Ä¢ nb slash pushes towards legitimate\n",
      " ‚Ä¢ nb www pushes towards legitimate\n",
      " ‚Ä¢ ratio digits url pushes towards legitimate\n",
      " ‚Ä¢ phish hints pushes towards legitimate\n",
      "\n",
      "üîç Concrete Evidence Found in Text:\n",
      " ‚Ä¢ LinkedIn‚Äôs User Agreement\n",
      " ‚Ä¢ Privacy Policy\n",
      " ‚Ä¢ Cookie Policy\n",
      " ‚Ä¢ Welcome to your professional community\n",
      " ‚Ä¢ Post your job for millions of people to see\n",
      " ‚Ä¢ Connect with buyers who have first-hand experience\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "# ---------- Load LGBM Model Package ----------\n",
    "with open(\"phishing_lgbm.pkl\", \"rb\") as f:\n",
    "    package = pickle.load(f)\n",
    "\n",
    "ml_model = package[\"model\"]\n",
    "scaler = package[\"scaler\"]\n",
    "features_list = package[\"features\"]\n",
    "lgbm_model = package[\"lgbm\"]\n",
    "\n",
    "# ---------- Feature Extractor ----------\n",
    "def extract_features(url):\n",
    "    feats = {}\n",
    "    feats['length_url'] = len(url)\n",
    "    feats['length_hostname'] = len(re.findall(r'://([^/]+)/?', url)[0]) if \"://\" in url else len(url)\n",
    "    feats['ip'] = 1 if re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', url) else 0\n",
    "    feats['nb_dots'] = url.count('.')\n",
    "    feats['nb_hyphens'] = url.count('-')\n",
    "    feats['nb_at'] = url.count('@')\n",
    "    feats['nb_qm'] = url.count('?')\n",
    "    feats['nb_and'] = url.count('&')\n",
    "    feats['nb_or'] = url.count('|')\n",
    "    feats['nb_eq'] = url.count('=')\n",
    "    feats['nb_underscore'] = url.count('_')\n",
    "    feats['nb_tilde'] = url.count('~')\n",
    "    feats['nb_percent'] = url.count('%')\n",
    "    feats['nb_slash'] = url.count('/')\n",
    "    feats['nb_star'] = url.count('*')\n",
    "    feats['nb_colon'] = url.count(':')\n",
    "    feats['nb_comma'] = url.count(',')\n",
    "    feats['nb_semicolumn'] = url.count(';')\n",
    "    feats['nb_dollar'] = url.count('$')\n",
    "    feats['nb_space'] = url.count(' ')\n",
    "    feats['nb_www'] = url.count('www')\n",
    "    feats['nb_com'] = url.count('.com')\n",
    "    feats['nb_dslash'] = url.count('//')\n",
    "    feats['http_in_path'] = 1 if \"http\" in url[url.find(\"://\")+3:] else 0\n",
    "    feats['https_token'] = 1 if \"https\" in url else 0\n",
    "    feats['ratio_digits_url'] = sum(c.isdigit() for c in url) / len(url)\n",
    "    feats['ratio_digits_host'] = 0.0\n",
    "    feats['punycode'] = 1 if \"xn--\" in url else 0\n",
    "    feats['shortening_service'] = 1 if re.search(r'bit\\.ly|goo\\.gl|tinyurl|ow\\.ly', url) else 0\n",
    "    feats['path_extension'] = 1 if re.search(r'\\.[a-zA-Z0-9]{2,4}(/|$)', url) else 0\n",
    "    feats['phish_hints'] = 1 if re.search(r'login|verify|bank|account|update|secure', url.lower()) else 0\n",
    "    feats['domain_in_brand'] = 0\n",
    "    feats['brand_in_subdomain'] = 0\n",
    "    feats['brand_in_path'] = 0\n",
    "    feats['suspecious_tld'] = 1 if re.search(r'\\.(zip|review|country|kim|cricket|science|work|party|info)$', url) else 0\n",
    "    return feats\n",
    "\n",
    "# ---------- SHAP Explainability Formatter ----------\n",
    "def format_shap_explanations(features_list, shap_array):\n",
    "    explanations_pos, explanations_neg = [], []\n",
    "    for feat, val in zip(features_list, shap_array[0]):\n",
    "        if abs(val) < 0.2:  # ignore weak contributions\n",
    "            continue\n",
    "        direction = \"phishing\" if val > 0 else \"legitimate\"\n",
    "        text = f\"{feat.replace('_',' ')} pushes towards {direction}\"\n",
    "        if val > 0:\n",
    "            explanations_pos.append(text)\n",
    "        else:\n",
    "            explanations_neg.append(text)\n",
    "    return explanations_pos, explanations_neg\n",
    "\n",
    "# ---------- Extract Main Content ----------\n",
    "def extract_main_content(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for element in soup([\"script\", \"style\", \"meta\", \"link\", \"header\", \"footer\", \"nav\", \"aside\", \"noscript\"]):\n",
    "        element.decompose()\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    return text\n",
    "\n",
    "# ---------- LLM Analysis (Ollama Mistral) ----------\n",
    "def analyze_with_ollama(content, model_name=\"mistral\"):\n",
    "    system_prompt = \"\"\"You are an expert cybersecurity analyst. \n",
    "Your task is to analyze the content of a website and determine if it is a scam, phishing attempt, or otherwise malicious.\n",
    "Respond STRICTLY in this JSON format:\n",
    "\n",
    "{\n",
    "  \"verdict\": \"phishing\" or \"legitimate\",\n",
    "  \"risk_level\": \"suspicious\" or \"safe\",\n",
    "  \"reasons\": [\"list of brief reasons\"],\n",
    "  \"evidence_snippets\": [\"list of concrete snippets found in the text\"]\n",
    "}\n",
    "\"\"\"\n",
    "    try:\n",
    "        user_message = f\"Analyze this website content:\\n\\n{content}\"\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': user_message}\n",
    "            ],\n",
    "            options={'temperature': 0.1}\n",
    "        )\n",
    "        response_text = response['message']['content'].strip()\n",
    "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            return json.loads(json_match.group())\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with Ollama: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------- Hybrid Classification ----------\n",
    "def classify_content(url):\n",
    "    # Step 1: Fetch page\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Failed to fetch URL: {e}\"}\n",
    "\n",
    "    # Step 2: Extract content\n",
    "    main_text = extract_main_content(html_content)\n",
    "    max_chars = 4000\n",
    "    text_for_llm = (url + \" \" + main_text)[:max_chars]\n",
    "\n",
    "    print(f\"üì° Feeding {len(text_for_llm)} characters to LLM (max={max_chars})\\n\")\n",
    "\n",
    "    # Step 3: ML prediction\n",
    "    feats = extract_features(url)\n",
    "    X_input = pd.DataFrame([feats]).reindex(columns=features_list, fill_value=0)\n",
    "    X_scaled = scaler.transform(X_input)\n",
    "    ml_prob = ml_model.predict_proba(X_scaled)[0][1]\n",
    "    ml_pred = \"phishing\" if ml_prob > 0.5 else \"legitimate\"\n",
    "\n",
    "    # SHAP explanations\n",
    "    explainer = shap.TreeExplainer(lgbm_model)\n",
    "    shap_values = explainer.shap_values(X_scaled)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_array = shap_values[1]\n",
    "    else:\n",
    "        shap_array = shap_values\n",
    "    ml_pos, ml_neg = format_shap_explanations(features_list, shap_array)\n",
    "\n",
    "    # Step 4: LLM prediction\n",
    "    llm_result = analyze_with_ollama(text_for_llm)\n",
    "    if llm_result:\n",
    "        llm_label = llm_result.get(\"verdict\", \"unknown\")\n",
    "        llm_risk = llm_result.get(\"risk_level\", \"suspicious\")\n",
    "        llm_reasons = llm_result.get(\"reasons\", [])\n",
    "        evidence_snippets = llm_result.get(\"evidence_snippets\", [])\n",
    "    else:\n",
    "        llm_label, llm_risk, llm_reasons, evidence_snippets = \"unknown\", \"suspicious\", [], []\n",
    "\n",
    "    # Step 5: Ensemble decision\n",
    "    llm_score = 1.0 if llm_label == \"phishing\" else 0.0 if llm_label == \"legitimate\" else 0.5\n",
    "    if llm_risk == \"suspicious\" and 0.15 < ml_prob < 0.5:\n",
    "        final = \"phishing\"\n",
    "    elif ml_prob >= 0.85:\n",
    "        final = \"phishing\"\n",
    "    elif ml_prob <= 0.15:\n",
    "        final = \"legitimate\"\n",
    "    else:\n",
    "        combined_score = (0.6 * ml_prob) + (0.4 * llm_score)\n",
    "        final = \"phishing\" if combined_score >= 0.5 else \"legitimate\"\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"final_verdict\": final,\n",
    "        \"confidence\": round(float(ml_prob), 3),\n",
    "        \"ml_prediction\": ml_pred,\n",
    "        \"ml_positive\": ml_pos,\n",
    "        \"ml_negative\": ml_neg,\n",
    "        \"llm_prediction\": llm_label,\n",
    "        \"llm_risk_level\": llm_risk,\n",
    "        \"llm_reasons\": llm_reasons,\n",
    "        \"evidence_snippets\": evidence_snippets\n",
    "    }\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main():\n",
    "    print(\"üåê Website Scam & Phishing Analyzer\")\n",
    "    print(\"----------------------------------------\")\n",
    "    website_url = input(\"Please enter the full URL to analyze: \").strip()\n",
    "    result = classify_content(website_url)\n",
    "\n",
    "    if \"error\" in result:\n",
    "        print(f\"‚ùå {result['error']}\")\n",
    "        return\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìã URL: {result['url']}\")\n",
    "    verdict_text = 'üî¥ PHISHING' if result['final_verdict'] == 'phishing' else 'üü¢ LEGITIMATE'\n",
    "    print(f\"‚öñÔ∏è Final Verdict: {verdict_text} (confidence={result['confidence']})\")\n",
    "    print(f\"‚úÖ ML Prediction: {result['ml_prediction']}\")\n",
    "    print(f\"ü§ñ LLM Prediction: {result['llm_prediction']} (risk={result['llm_risk_level']})\")\n",
    "\n",
    "    # Unified Analysis\n",
    "    print(\"\\nü§ñ AI's Contextual Analysis:\")\n",
    "\n",
    "    print(\"\\nPhishing Indicators:\")\n",
    "    for r in result['ml_positive'] + result['llm_reasons']:\n",
    "        print(f\" ‚Ä¢ {r}\")\n",
    "    if not result['ml_positive'] and not result['llm_reasons']:\n",
    "        print(\" ‚Ä¢ None detected.\")\n",
    "\n",
    "    print(\"\\nLegitimate Indicators:\")\n",
    "    for r in result['ml_negative']:\n",
    "        print(f\" ‚Ä¢ {r}\")\n",
    "    if not result['ml_negative']:\n",
    "        print(\" ‚Ä¢ None detected.\")\n",
    "\n",
    "    print(\"\\nüîç Concrete Evidence Found in Text:\")\n",
    "    if result['evidence_snippets']:\n",
    "        for e in result['evidence_snippets']:\n",
    "            print(f\" ‚Ä¢ {e}\")\n",
    "    else:\n",
    "        print(\" ‚Ä¢ None detected.\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df724f8-6e0e-4c1d-86b5-5b5d88b39b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Website Scam & Phishing Analyzer\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the full URL to analyze:  https://www.linkedin.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° Feeding 4000 characters to LLM (max=4000)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh\\anaconda3\\Lib\\site-packages\\shap\\explainers\\_tree.py:583: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìã URL: https://www.linkedin.com\n",
      "‚öñÔ∏è Final Verdict: üü¢ LEGITIMATE (confidence=0.224)\n",
      "‚úÖ ML Prediction: legitimate\n",
      "ü§ñ LLM Prediction: legitimate (risk=safe)\n",
      "\n",
      "ü§ñ AI's Contextual Analysis:\n",
      "\n",
      "Legitimate Indicators:\n",
      " ‚Ä¢ length url pushes towards legitimate.\n",
      " ‚Ä¢ length hostname pushes towards legitimate.\n",
      " ‚Ä¢ nb slash pushes towards legitimate.\n",
      " ‚Ä¢ nb www pushes towards legitimate.\n",
      " ‚Ä¢ ratio digits url pushes towards legitimate.\n",
      " ‚Ä¢ phish hints pushes towards legitimate.\n",
      "\n",
      "LLM Reasons for Legitimacy:\n",
      " ‚Ä¢ The website is well-known and reputable (LinkedIn).\n",
      " ‚Ä¢ The URL structure follows the expected pattern for LinkedIn.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "# ---------- Load LGBM Model Package ----------\n",
    "with open(\"phishing_lgbm.pkl\", \"rb\") as f:\n",
    "    package = pickle.load(f)\n",
    "\n",
    "ml_model = package[\"model\"]\n",
    "scaler = package[\"scaler\"]\n",
    "features_list = package[\"features\"]\n",
    "lgbm_model = package[\"lgbm\"]\n",
    "\n",
    "# ---------- Feature Extractor ----------\n",
    "def extract_features(url):\n",
    "    feats = {}\n",
    "    feats['length_url'] = len(url)\n",
    "    feats['length_hostname'] = len(re.findall(r'://([^/]+)/?', url)[0]) if \"://\" in url else len(url)\n",
    "    feats['ip'] = 1 if re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', url) else 0\n",
    "    feats['nb_dots'] = url.count('.')\n",
    "    feats['nb_hyphens'] = url.count('-')\n",
    "    feats['nb_at'] = url.count('@')\n",
    "    feats['nb_qm'] = url.count('?')\n",
    "    feats['nb_and'] = url.count('&')\n",
    "    feats['nb_or'] = url.count('|')\n",
    "    feats['nb_eq'] = url.count('=')\n",
    "    feats['nb_underscore'] = url.count('_')\n",
    "    feats['nb_tilde'] = url.count('~')\n",
    "    feats['nb_percent'] = url.count('%')\n",
    "    feats['nb_slash'] = url.count('/')\n",
    "    feats['nb_star'] = url.count('*')\n",
    "    feats['nb_colon'] = url.count(':')\n",
    "    feats['nb_comma'] = url.count(',')\n",
    "    feats['nb_semicolumn'] = url.count(';')\n",
    "    feats['nb_dollar'] = url.count('$')\n",
    "    feats['nb_space'] = url.count(' ')\n",
    "    feats['nb_www'] = url.count('www')\n",
    "    feats['nb_com'] = url.count('.com')\n",
    "    feats['nb_dslash'] = url.count('//')\n",
    "    feats['http_in_path'] = 1 if \"http\" in url[url.find(\"://\")+3:] else 0\n",
    "    feats['https_token'] = 1 if \"https\" in url else 0\n",
    "    feats['ratio_digits_url'] = sum(c.isdigit() for c in url) / len(url)\n",
    "    feats['ratio_digits_host'] = 0.0\n",
    "    feats['punycode'] = 1 if \"xn--\" in url else 0\n",
    "    feats['shortening_service'] = 1 if re.search(r'bit\\.ly|goo\\.gl|tinyurl|ow\\.ly', url) else 0\n",
    "    feats['path_extension'] = 1 if re.search(r'\\.[a-zA-Z0-9]{2,4}(/|$)', url) else 0\n",
    "    feats['phish_hints'] = 1 if re.search(r'login|verify|bank|account|update|secure', url.lower()) else 0\n",
    "    feats['domain_in_brand'] = 0\n",
    "    feats['brand_in_subdomain'] = 0\n",
    "    feats['brand_in_path'] = 0\n",
    "    feats['suspecious_tld'] = 1 if re.search(r'\\.(zip|review|country|kim|cricket|science|work|party|info)$', url) else 0\n",
    "    return feats\n",
    "\n",
    "# ---------- SHAP Explainability Formatter ----------\n",
    "def format_shap_explanations(features_list, shap_array, prediction_type):\n",
    "    explanations = []\n",
    "    for feat, val in zip(features_list, shap_array[0]):\n",
    "        if abs(val) < 0.2:  # ignore weak contributions\n",
    "            continue\n",
    "        direction = \"phishing\" if val > 0 else \"legitimate\"\n",
    "        if prediction_type == \"legitimate\" and val < 0:\n",
    "            text = f\"{feat.replace('_',' ')} pushes towards legitimate.\"\n",
    "            explanations.append(text)\n",
    "        elif prediction_type == \"phishing\" and val > 0:\n",
    "            text = f\"{feat.replace('_',' ')} pushes towards phishing.\"\n",
    "            explanations.append(text)\n",
    "    return explanations\n",
    "\n",
    "# ---------- Extract Main Content ----------\n",
    "def extract_main_content(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for element in soup([\"script\", \"style\", \"meta\", \"link\", \"header\", \"footer\", \"nav\", \"aside\", \"noscript\"]):\n",
    "        element.decompose()\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    return text\n",
    "\n",
    "# ---------- LLM Analysis (Ollama Mistral) ----------\n",
    "def analyze_with_ollama(content, model_name=\"mistral\"):\n",
    "    system_prompt = \"\"\"You are an expert cybersecurity analyst. \n",
    "Your task is to analyze the content of a website and determine if it is a scam, phishing attempt, or otherwise malicious.\n",
    "Respond STRICTLY in this JSON format:\n",
    "\n",
    "{\n",
    "  \"verdict\": \"phishing\" or \"legitimate\",\n",
    "  \"risk_level\": \"suspicious\" or \"safe\",\n",
    "  \"reasons\": [\"list of brief reasons\"],\n",
    "  \"evidence_snippets\": [\"list of concrete snippets found in the text\"]\n",
    "}\n",
    "\"\"\"\n",
    "    try:\n",
    "        user_message = f\"Analyze this website content:\\n\\n{content}\"\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': user_message}\n",
    "            ],\n",
    "            options={'temperature': 0.1}\n",
    "        )\n",
    "        response_text = response['message']['content'].strip()\n",
    "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            return json.loads(json_match.group())\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with Ollama: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------- Hybrid Classification ----------\n",
    "def classify_content(url):\n",
    "    # Step 1: Fetch page\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Failed to fetch URL: {e}\"}\n",
    "\n",
    "    # Step 2: Extract content\n",
    "    main_text = extract_main_content(html_content)\n",
    "    max_chars = 4000\n",
    "    text_for_llm = (url + \" \" + main_text)[:max_chars]\n",
    "\n",
    "    print(f\"üì° Feeding {len(text_for_llm)} characters to LLM (max={max_chars})\\n\")\n",
    "\n",
    "    # Step 3: ML prediction\n",
    "    feats = extract_features(url)\n",
    "    X_input = pd.DataFrame([feats]).reindex(columns=features_list, fill_value=0)\n",
    "    X_scaled = scaler.transform(X_input)\n",
    "    ml_prob = ml_model.predict_proba(X_scaled)[0][1]\n",
    "    ml_pred = \"phishing\" if ml_prob > 0.5 else \"legitimate\"\n",
    "\n",
    "    # SHAP explanations\n",
    "    explainer = shap.TreeExplainer(lgbm_model)\n",
    "    shap_values = explainer.shap_values(X_scaled)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_array = shap_values[1]\n",
    "    else:\n",
    "        shap_array = shap_values\n",
    "    ml_explanations = format_shap_explanations(features_list, shap_array, ml_pred)\n",
    "\n",
    "    # Step 4: LLM prediction\n",
    "    llm_result = analyze_with_ollama(text_for_llm)\n",
    "    if llm_result:\n",
    "        llm_label = llm_result.get(\"verdict\", \"unknown\")\n",
    "        llm_risk = llm_result.get(\"risk_level\", \"suspicious\")\n",
    "        llm_reasons = llm_result.get(\"reasons\", [])\n",
    "        evidence_snippets = llm_result.get(\"evidence_snippets\", [])\n",
    "    else:\n",
    "        llm_label, llm_risk, llm_reasons, evidence_snippets = \"unknown\", \"suspicious\", [], []\n",
    "\n",
    "    # Step 5: Ensemble decision\n",
    "    llm_score = 1.0 if llm_label == \"phishing\" else 0.0 if llm_label == \"legitimate\" else 0.5\n",
    "    if llm_risk == \"suspicious\" and 0.15 < ml_prob < 0.5:\n",
    "        final = \"phishing\"\n",
    "    elif ml_prob >= 0.85:\n",
    "        final = \"phishing\"\n",
    "    elif ml_prob <= 0.15:\n",
    "        final = \"legitimate\"\n",
    "    else:\n",
    "        combined_score = (0.6 * ml_prob) + (0.4 * llm_score)\n",
    "        final = \"phishing\" if combined_score >= 0.5 else \"legitimate\"\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"final_verdict\": final,\n",
    "        \"confidence\": round(float(ml_prob), 3),\n",
    "        \"ml_prediction\": ml_pred,\n",
    "        \"ml_explanations\": ml_explanations,\n",
    "        \"llm_prediction\": llm_label,\n",
    "        \"llm_risk_level\": llm_risk,\n",
    "        \"llm_reasons\": llm_reasons,\n",
    "        \"evidence_snippets\": evidence_snippets\n",
    "    }\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main():\n",
    "    print(\"üåê Website Scam & Phishing Analyzer\")\n",
    "    print(\"----------------------------------------\")\n",
    "    website_url = input(\"Please enter the full URL to analyze: \").strip()\n",
    "    result = classify_content(website_url)\n",
    "\n",
    "    if \"error\" in result:\n",
    "        print(f\"‚ùå {result['error']}\")\n",
    "        return\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìã URL: {result['url']}\")\n",
    "    verdict_text = 'üî¥ PHISHING' if result['final_verdict'] == 'phishing' else 'üü¢ LEGITIMATE'\n",
    "    print(f\"‚öñÔ∏è Final Verdict: {verdict_text} (confidence={result['confidence']})\")\n",
    "    print(f\"‚úÖ ML Prediction: {result['ml_prediction']}\")\n",
    "    print(f\"ü§ñ LLM Prediction: {result['llm_prediction']} (risk={result['llm_risk_level']})\")\n",
    "\n",
    "    # Unified Analysis for phishing or legitimate verdict\n",
    "    print(\"\\nü§ñ AI's Contextual Analysis:\")\n",
    "\n",
    "    if result['final_verdict'] == \"phishing\":\n",
    "        print(\"\\nPhishing Indicators:\")\n",
    "        for explanation in result['ml_explanations']:\n",
    "            print(f\" ‚Ä¢ {explanation}\")\n",
    "        print(\"\\nLLM Evidence Snippets:\")\n",
    "        for snippet in result['evidence_snippets']:\n",
    "            print(f\" ‚Ä¢ {snippet}\")\n",
    "    else:\n",
    "        print(\"\\nLegitimate Indicators:\")\n",
    "        for explanation in result['ml_explanations']:\n",
    "            print(f\" ‚Ä¢ {explanation}\")\n",
    "        print(\"\\nLLM Reasons for Legitimacy:\")\n",
    "        for reason in result['llm_reasons']:\n",
    "            print(f\" ‚Ä¢ {reason}\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac3cdde-27b8-430c-a222-c843ee2a5cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (virtual_mouse)",
   "language": "python",
   "name": "virtual_mouse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
